\documentclass[10pt]{article}
\usepackage{amsmath}
\usepackage{stmaryrd}
\usepackage{amssymb}
\usepackage{interval}
\usepackage[ruled]{algorithm2e}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newcommand{\Not}[1]{$\neg #1$}
\newcommand{\dbrackets}[1]{\llbracket#1\rrbracket}
\newcommand{\numberthis}{\addtocounter{equation}{1}\tag{\theequation}}
\intervalconfig{
soft open fences,
separator symbol =;,
}

\newcommand{\umsg}[3]{u_{#1\rightarrow#2}^{#3}}
\newcommand{\hmsg}[3]{h_{#1\rightarrow#2}^{#3}}

\usepackage[a4paper, total={6in, 8in}]{geometry}
\begin{document}
\section{Global Heuristics}
In the previous section we demonstrated local heuristics, that can be very efficiently implemented and one can rigorously analyze them. However, this benefits come with a cost of a significantly weaker performance on larger ratios of clauses to variables. In this section, we will present global heuristics, algorithms that in their free steps make decisions motivated by a global structure of the formula, rather than a local neighborhood. Currently, the most popular such heuristics are Belief Propagation (BP) and Survey Propagation (SP), both exhibiting a very good performance on large formulas with ratio close to the conjectured threshold of $4.2667$. (Citation needed)
(Maybe where to they come from)
\section{Belief Propagation}
First, let's define a uniform distribution over the set of all satysfying assignments of given 3-CNF $\phi$. Then, this distribution can be written down:
\begin{equation}
\label{eq:uniform}
\mu(\mathcal{A}) = \frac{1}{Z} \mathcal{A} \dbrackets{\phi}
\end{equation}
where Z is the total number of satysfying assignments. 
\par 
BP aims at computing marginals of this distribution, namely $\mu(\mathcal{A} \mid \mathcal{A} \dbrackets{x} = b)$ for $b = 0, 1$. The algorithm itself is a message passing algorithm, that is, it procceds in rounds, where in each round the messages between clauses and variables are updated according to the equations below:

\begin{align}
    \hmsg{i}{a}{r+1} &= \tanh{ \Biggl\{ \sum_{b \in {\partial_+}i(a)} \umsg{b}{i}{r} - \sum_{b \in {\partial_-}i(a)} \umsg{b}{i}{r} \Biggr\} }
    \numberthis \label{eq:messages:h} \\
    \umsg{a}{i}{r+1} &= - \frac{1}{2} \log{ \Biggl\{ 1 - \prod_{j \in \partial a  i}  \biggl(\frac{1 - \hmsg{j}{a}{r}}{2} \biggr) \Biggr\}}
    \numberthis \label{eq:message:u}
\end{align}

The message $\umsg{a}{i}{r}$ is usually interpreted as a warning sent from clause $a$ to variable $i$, and it informs variable $i$ how much do the other variables inside clause $a$ lean towards a polariry that violates $a$. With this interpretation in mind, the message $\hmsg{i}{a}{r}$ is treated as the probability that the variable $i$ satisfies the clause $a$.
\par
The algorithm is presented in Algorithm~\ref{alg:bp}. BP terminates when the messages converge, i.e.~when the messages do not change significantly between two consecutive rounds, or when the number of rounds exceeds some predefined limit referred to by $maxiter$. Afterwards, the approximation to the marginals is computed as follows:
\begin{equation}
    \mathcal{V}_x^r(b) = \frac{1}{2} \biggl(1 + {(-1)}^b \tanh \biggl\{ \sum_{a \in \partial_- i} \umsg{a}{i}{r} - \sum_{a \in \partial_+ i} \umsg{a}{i}{r} \biggr\} \biggr)
\end{equation}

\begin{algorithm}
    \caption{Belief Propagation}\label{alg:bp}
    \KwIn{A k-SAT formula $\phi$ and messages $\umsg{a}{i}{0}$ and $\hmsg{i}{a}{0}$}
    \KwOut{Approximations ${\{\mathcal{V}_{x_i}(b)\}}_{i = 1}^{n}$}
    \For{$r = 1, \ldots , maxiter$}{
        Calculate messages $\umsg{a}{i}{r}$ and $\hmsg{i}{a}{r}$ according to equations~\ref{eq:messages:h} and~\ref{eq:message:u}\;
        Compute the approximations $\mathcal{V}_x(b)$\;
        \If{$\max_{x, b} | \mathcal{V}_x^r(b)  - \mathcal{V}_x^{r-1}(b) | < \epsilon$}{Break}
    }
    \Return{$ \{{\mathcal{V}_x(b)}\}_{i = 0}^n$}
\end{algorithm}

\par
While for trees BP converges to the exact marginals, for general graphs it only approximates it (Citation needed). However, in the limit $N \rightarrow \infty$ the underlaying graph tends to a random tree, which explains why the performance of BP scales with the number of variables.
Still, it provides us with a useful insight to the structure of the satysfying assignments, that we can exploit to make more informed decisions in the free steps, which is depicted in the Algorithm\ref{alg:bpd}.
\begin{algorithm}
\caption{Belief Propagation Inspired Decimation}\label{alg:bpd}
\KwIn{A k-SAT formula $\phi$}
\KwOut{\textit{True} if $\phi$ is satisfiable, \textit{False} otherwise}
\For{$t = 1, \ldots , n$}{
    \eIf{there exists pure literal $l$ or unit clause $\{ l \}$}{
        satisfy $l$\;
    }{
        Compute the approximation of the marginals ${\{\mathcal{V}_{x_i}(b)\}}_{i = 1}^{n}$ by running  BP on $\phi$ \;
        Choose the most biased variable $x^*$, i.e. $x^* = \arg \max_{x} | \mathcal{V}_x(0) - \mathcal{V}_x(1) |$\;
        Set $x^*$ to $b \in \{0, 1\}$ that maximizes $\mathcal{V}_{x^*}(b)$ and adjust $\phi$ accordingly\;
    }
}
\end{algorithm}

\subsection*{Time Complexity}
From previous section about PC and UC, we know that the steps taken by these heuristics run in $O(N k \alpha)$, so we only need to analyze the complexity of Belief Propagation algorithm. First, we note that a single round of updates to the messages takes time $O(N k \alpha)$, as we need to essentially iterate over all clauses. Hence, BP runs in $O(r N k \alpha)$, where $r$ is the number of rounds. In fact, numerical experiments as well as theoretical considerations suggests that $r = O(\log N)$, which is motivated by the intuition that the expected distance between two nodes in our graph is $O(\log N)$. Hence, the overall time complexity of the heuristic is $O(N^2 k \alpha \log N)$.
\par  
In my implementation, I set $maxiter$ to $200$ and $\epsilon$ to $10^{-6}$. Moreover, instead of running BP on each free step, I run it only having satisfied $\lceil f * N_t \rceil$ varaibles since previous propagation. It boosts the time complexity to $O(N k \alpha \log^2 N)$, as BP is invoked only $O(\log N)$ times.





\end{document}